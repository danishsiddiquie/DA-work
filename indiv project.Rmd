---
title: "individual"
author: "Danish Siddiquie"
date: "9 December 2017"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}

library(dplyr)
library(ggplot2)
library(Hmisc)
library(readr)
library(ggthemes)
library(knitr)
library(xtable)
library(pander)
library(MASS)
library(car)
library(ggmap)
library(tidyr)
library(lubridate)
library(rmarkdown)

vehicle_draft<-read.csv("/Users/apple/Desktop/DA working directory/final project/NYPD_Motor_Vehicle_Collisions.csv")
```

##Part 1

###1. Summary
```{r,echo=TRUE}

summary(vehicle_draft$LATITUDE)
summary(vehicle_draft$LONGITUDE)
```

Code chunk demonstrates the summary function.
The function typically is used to to show the min, max, quartiles, and N/A values in each column of the data set. 
For ym example, provides a summary for the LATITUDE and LONGITUDE column in vehicle_draft dataset.The command takes in the column of the data set, and provdes the min,max quartiles, and NA values in the column.I am using this to figure out the N/A values, and the range of coordinates in the data set. 


###2. Glimpse
```{r,echo=TRUE}
glimpse(vehicle_draft$BOROUGH)
```

The code chunk above demonstrates the glimpse() command. The command works by inputing a dataset into the function. It then shows you every column in the dataset and next to the column names lists the beginning values, or variables that appear first in that column. It would typically be used to take a quick look at the dataset you are working with.
In my example, the code shows us the factor levels and a glimpse of those factors in the BOROUGH column. I will be using this to see if it has the correct no. of Boroughs [6], or has any extra blanks I should filter out


###3. Filter
###4. Remova N/A's
```{r,echo=FALSE}
vehicle<-vehicle_draft%>%filter(LATITUDE>39 & LATITUDE<45 & LONGITUDE>"-70" & LONGITUDE<"-75", 
                                BOROUGH!="",
                                !is.na(BOROUGH))
```

The filter function takes in the requirements we need, and filters out all the rest. The function is typically used to filter the data set as per the analysis requirement.
In my example, the code creates new data set which filters our unnecessary coordinates, blank columns and N/A values in Boroughs.we use this code to remove all the unnecessary values that may affect our analysis, since we only need the correct range of coordinates [new york coordinates], and do not require N/A values.


###5. str
```{r,echo=TRUE}
str(vehicle$NUMBER.OF.PEDESTRIANS.INJURED)
```

The str function takes in the column of which which you want to know the string type of. We typically use this to make sure it is of the correct type of string for the given column variable.
For my example,The code checks the string type of the column Pedestrians Injured. In this case, the number of pedestrians injured--which should be in integers-- is in integers. I would require this to be in integers for further analysis.


###6. Mutate
###7. ifelse
```{r}
vehicle <- vehicle%>%
  mutate(killed = ifelse(NUMBER.OF.PERSONS.KILLED > 0 | 
                                      NUMBER.OF.PEDESTRIANS.KILLED > 0 | 
                                      NUMBER.OF.CYCLIST.KILLED > 0 |
                                      NUMBER.OF.MOTORIST.KILLED > 0, 1, 0), 
         numkilled = ifelse(killed == 1, 
                        as.numeric(NUMBER.OF.PERSONS.KILLED) + 
                        as.numeric(NUMBER.OF.CYCLIST.KILLED) +
                        as.numeric(NUMBER.OF.PEDESTRIANS.KILLED) +
                        as.numeric(NUMBER.OF.MOTORIST.KILLED), 0),
         injured = ifelse(NUMBER.OF.PERSONS.INJURED > 0 | 
                        NUMBER.OF.PEDESTRIANS.INJURED > 0 | 
                        NUMBER.OF.CYCLIST.INJURED > 0 |
                        NUMBER.OF.MOTORIST.INJURED > 0, 1, 0),
         numinjured = ifelse(injured == 1, 
                        as.numeric(NUMBER.OF.PERSONS.INJURED) + 
                        as.numeric(NUMBER.OF.CYCLIST.INJURED) +
                        as.numeric(NUMBER.OF.PEDESTRIANS.INJURED) +
                        as.numeric(NUMBER.OF.MOTORIST.INJURED), 0))

```

The mutate function takes in the name of the new column we want to create, and equates it to the desired function we want to run to get values for the column. In this case, we are using an ifelse function.
The ifelse function takes in the condition, the command for if it is true, and the command for "else". 
The code creates 4 different columns using the mutate function. To create the columns, however, we have some conditions, and hence we use an ifelse functions.
For our example, for the "killed" column, we are trying to figure out if anybody died in the accident. if any of the "KILLED" categories [pedestrian.killed OR cyclist.killed OR person.killed etc] are >0, then the killed column shows "1", as an affirmation, or else shows zero, showing no one died. 
For the "numkilled" column, if for the particular accident, the "killed" columns shows "1", then it adds the actual no. of people killed in each column, and shows the value in the column. The as.numeric makes sure that all the values are numeric values. 
The same procedure is repeated for injured and numinjured, except the "INJURED" columns are used for these.
This code helps us find the no. of people getting injured or killed in an accident, which will help us create regression models using the same. 


###8. Rename
```{r}
vehicle<-rename(vehicle,accident_factor=CONTRIBUTING.FACTOR.VEHICLE.1)
```

The code takes in the data set, the new column name, and equates it to the name of the column you want to change. It is typically used to change a column name to a name which would make more sense while analysing data.
In our example, The code renames the "CONTRIBUTING.FACTOR.VEHICLE.1" column to "accident_factor" since it makes more sense, and is easier to use while analysing.


###9. summarise
###10. group_by
```{r}

df<-data.frame(PERSONS.INJURED=c(vehicle$NUMBER.OF.PERSONS.INJURED),
               PERSONS.KILLED=c(vehicle$NUMBER.OF.PERSONS.KILLED),
               PEDESTRIANS.INJURED=c(vehicle$NUMBER.OF.PEDESTRIANS.INJURED),
               PEDESTRIANS.KILLED=c(vehicle$NUMBER.OF.PEDESTRIANS.KILLED),
               MOTORIST.INJURED=c(vehicle$NUMBER.OF.MOTORIST.INJURED),
               MOTORIST.KILLED=c(vehicle$NUMBER.OF.MOTORIST.KILLED),
               CYCLIST.INJURED=c(vehicle$NUMBER.OF.CYCLIST.INJURED),
               CYCLIST.KILLED=c(vehicle$NUMBER.OF.CYCLIST.KILLED))
               

df_long<-df%>%
  gather("fatality",factor_key=TRUE)

total_fatality<-df_long%>%group_by(fatality)%>%
  summarise(total=sum(value))

```

The summarise function usually goes hand in hand with the group_by function. The group_by function groups the data set by different factors being repeated in a column, and the summarise function summarise those grouped factors by another variable [in our caase, the no. of accidents]

The entire code helps us create a summary table for the total no. of accidents occuring for each column of killed and injured category. Since all the variables I need are different columns, to use group_by, I will have to create a new long form data frame which has all the columns as rows. 
The df data set first filters out all the required columns that I need. 
the df_long variable uses "gather". Gather takes multiple columns and collapses them into key-value pairs, duplicating all other columns as needed. the factor_key=TRUE will store key values as a factor, which preserves the original ordering of the columns.
The total_fatality groups the rows by the fatality column in df_long [which were all the category columns in in the original data set] and summarises them by total no. of those accidents occuring.



##Part 2


```{r}
vehicle <- vehicle%>%
  mutate(hour = hour(hm(TIME)))
```

creates a new column called hour which extracts the hour from the hour:minute format in the time column.



### Bar with conf. int (no grouping variable since long form data set. Another bar plot below)
```{r}
vehicle%>%ggplot(aes(hour,numinjured,fill="aquamarine4"))+
  stat_summary(fun.y = "mean",geom="bar")+
  stat_summary(fun.data = "mean_cl_normal",geom="errorbar",
               fun.args = list(conf.int=0.95),width=0.35)+
  theme(legend.position='none')+
  labs(y="average no. of injured people in an accident")
```

The code creates a bar plot to show the no. of accidents occuring for each category/fatality [pedestrian killed, pedestrian injured etc.]. ggplot takes in the data set, and sets the x and y axis. geom_bar creates the bar plot. Inside the aesthetics, the "fill" functions fills different color for each fatality. We want to specify that the height of the column equals the no. of accidents occuring for the discrete variable. Therefore, we use stat="identity". The axis.test rotates the fatality names by 60 degrees and centre aligns them under the bar. This code would help me and the audience visualise the difference in the frequency of fatalities.
The plot shows that the most no. of people that actually get injured is the person driving. And injured count is signficantly higher than killed count for all fatalities. 



```{r,echo=FALSE,warning=FALSE}
myMap<-get_map("New York,ny",zoom=11,maptype="roadmap")

```


The code generates a variable which would contain all our specified requirements of the desired New York city map. get_map function helps us provide specifications to out desired map, and takes the help of google maps to do that. 
"New York, ny" is a map already stored in their data base, and the zoom helps us zoom in.
Maptype provides us with the type of map we need, in this case a road map, since we want to see different streets and Boroughs in the city. 


###1. Map
```{r,echo=FALSE,warning=FALSE}
ggmap(myMap) + #ggmap takes data from the myMap specifications we made
  geom_point(data=vehicle, aes(LONGITUDE, LATITUDE, color = BOROUGH), alpha = .006, size = 1) + 
  #geom_point jots points using Latitude Longitude coordinates given in datset "vehicle", and uses color depending on which borough the accident happened. The opaqueness of the points are 0.006, and the size is 1.
  guides(color = guide_legend(override.aes = list(alpha = 1, size = 3)))+
  #had to overridde the legend to show the actual color of the points, and not the faded color.
  labs(title="Map showing distinct Boroughs")+
  theme(plot.title = element_text(hjust = .5)) #adjusts the title to be centre aligned.
```

The code will create a map with Borough highlighted in different colours [using the coordinates of accidents as points on the map]. This will help us identify each borough for our visual analysis. The aesthetics in the geom_point map the coordinates, and the color seperates the color of the point by each Borough. Alpha changes the opaqueness of the points, I have kept it low for mine since a lot of points are concentrated in the same area. The size of the points are 1.
The guide_legend provides me an override on the legend so I can show the actual color used for each borough, and not show the faded color, since it would be too light for the audience to distinguish.


###2. Frequency Density
```{r}
ggplot(vehicle, aes(x=format(as.POSIXct(as.character(vehicle$TIME), format = "%H:%M"), "%H:%M"))) + 
  geom_density(aes(group=BOROUGH,col=BOROUGH)) +
  scale_x_discrete(breaks=c('00:00', '04:00','08:00','12:00','16:00','20:00')) +
  labs(x='Time of day',title="Density plot of the density of acciddents occuring during each time of the day")
```

The code creates a density plot which shows the amount of accidents happening at different time of the day, and shows a different density for each borough.
POSIXct is a function to manipulate objects of classes date and time. code interpets the format in the time column, and uses it as the x variable.
geom_point creates a density plot with a different density for each Borough. 
By default, the group is set to the interaction of all discrete variables in the plot. This often partitions the data correctly, but when it does not, or when no discrete variable is used in the plot, you will need to explicitly define the grouping structure, by mapping group to a variable that has a different value for each group. Hence, I had to use the "group=" function to specify that I want to group by Borough.
scale_x_discrete breaks the x axis in such a way that we can see these values being seen on the x axis rather than any random no. which would not look like different hours during the day.


###3. Histogram
```{r}
ggplot(vehicle, aes(as.numeric(hour)))+
  geom_histogram(fill="aquamarine4",col="black")+
  theme_calc()+
  facet_wrap(~BOROUGH)+
  labs(x='Hour of day',y='no. of accidents',title='barplot of no. of accidents at each hour of the day')+
  theme(plot.title = element_text(hjust = .5))

```

the code creates histogram faceted for different boroughs, and shows the no. of accidents happening in each borough during each hour of the day.
It is usually used to show a polished plot of data found or create a plot in hopes of discovering new thigns about the data
ggplot makes the hour [as a numeriv value] as the x axis.
geom_histogram creates a histogram with an appealing color, and a black outline.
This plot helps us answer the no. of accidents occuring at different hours of the day in each Borough.


###4. Boxplot
```{r}
ggplot(vehicle,aes(x=BOROUGH,y=numinjured))+
  geom_boxplot(notch=TRUE)
```

The code creates a boxplot for the no. of injured column vs each Borough.
The boxplot is typically used to show a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum
In our example, The notch=TRUE shows that if two boxes' notches do not overlap this is ‘strong evidence’ their medians differ. However, it doesn't really us, since our median is almost 0 [tried using different y limits, still got the same result. This makes sense since most of the accidents have no fatalities at all. The points above are just outliers, which show that there are some rare scenarios where a lot of people got injured. 



###5. Bar plot with grouping variables
```{r,warning=FALSE}
ggplot(vehicle,aes(hour))+
  geom_bar(aes(fill=accident_factor))+
  facet_wrap(~BOROUGH)+
  theme_calc()+
  labs(x="Borough",y="No. of accidents", title="Barplot of no. of accidents \n in each Borough")+
  theme(plot.title = element_text(hjust = .5),legend.position = "none")
```

code that creates a bar plot for no. of accidents occuring during each hour of the day, with different accident factors as colours on the bar, faceted for each Borough. 
I had to remove the legend since there were A LOT of accident factors, so much so that if I showed the legend, the bar graph would go out of picture. That is why I used the legend.position='none' function.
This plot does not really help us since there are too many variables for us to distinguish, most of which is "unspecified". 


###6. Frequency Polygon
```{r,warning=FALSE}
ggplot(vehicle,aes(hour))+
  geom_freqpoly(aes(col=BOROUGH))

```

The command works by having an input of a dataset and being told what the aestetics are. It would typically be used to create a polished visual of the frequency of a variable in relation to another
Creates a frequency polygon showing the frequency of accidents occuring during each hour of the day, and a separate line and color for each Borough. 


##Part 3


###1. Bivariate linear regression
```{r}
#creates a bivariate linear regression model for the number of people killed vs Borough
model_hour <- lm(data=vehicle, 
            numkilled~hour)
summary(model_hour)
```

The function is typically used to determine the extent to which there is a linear relationship between a dependent variable and one or more independent variables. In Bivariate linear regression, a single independent variable is used to predict the value of a dependent variable.
In our example, The code creates a bivariate linear regression model to see the affect of hour on the number of people killed in an accident. The r-sqaured value is extremely low, which shows that just almost 0.01 percent of the numkilled is explained by the hour variable. However, the r-value for any or all factors combined would still be low [will see in the next model], since it is impossible to explain all the causes for a random variable that can be affected by an n number of factors; factors we might not even have the data for. 
The p values is <0.05, therefore it is statistically significant. The coefficient suggests that an increase in hour decreases the no. of accidents [since a negative slope].


### 2. Multivariate linear regression
```{r}
model_multi <- lm(data=vehicle, 
            numkilled~as.factor(BOROUGH)+hour+accident_factor)
summary(model_multi)
```

In Multivariate regression, a multiple independent variables are used to predict the value of a dependent variable.
In our example, the code creates a multivariate linear regression model for the number of pedestrian accidents vs [Borough and hour and each factor in the accident_factor column]. The r-sqaured value, even though higher than the last model, is as expected extremely low. We can see that some factors stand out more than other [in regards to p-value], and only they are statsitically significant. I would use only those factors as statistically relevant factors in an accident, since they qualitatively make sense as well. [we will create a table of statistically significant values in the group project]


### 3. Step AIC model
```{r}
stepmodel_multi <- stepAIC(model_multi)
stepmodel_multi$anova
```

The function takes multiple combinations of subsets of linear independent variable to find the best model for our regression test. 
The code creates a step AIC model. The step AIC model selects a subset of predictor varibales froma  larger set. the anova function comapres and suggests the best model. 
We can see that the best model is the same as the initial model we started with. 

### 4. Residual plot
```{r}
vehicle<-vehicle%>%
  mutate(residual=resid(model_multi))

ggplot(vehicle, aes(hour, residual)) +
  geom_point() +
  stat_smooth(method = "lm") +
  ylim(-0.02,0.02)+
  labs(x="hour", y="Residual", title="Residual Plot Distribution of Residuals vs. BOROUGH") +
  theme(plot.title = element_text(hjust = .5))
```

A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate.

The code first creates a residual column using the mutate and resid function for the linear regression model. The code then creates a ggplot with x axis as hour [a variable used in the linear regression] and y axis as the residuals. The code also makes a best fit line across the points.The ylim is set between very smal ranges since I can see from the table and summary of residuals that max and min range. 

The distribution of points is extremely linear, however not evenly distributed around x axis and more towards the negative side, suggesting that a non-linear model would be better to use.



### 5. Residual histogram
```{r}
ggplot(vehicle,aes(residual))+
  geom_histogram(col="white",fill="aquamarine4",binwidth = 0.0005)+
  labs(x="Residual", y="Frequency", title="Histogram of the frequency of residual") +
  xlim(-0.005,0.005)+
  theme(plot.title = element_text(hjust = .5))
```

The plot is used to see if our data is distributed normally or not, and to see if a linear regression can be thus used to explain relation between two variables or not. 
The code creates a histogram for the residuals to see the frequency. I set the binwidth and range to a small number because the values of residuals are extremely small.
The histogram shows that the residuals are completely skewed to the left, suggesting that the data is not normal. A histogram of residuals with a larger concentration of values near zero would suggest a more consistent dataset, and more reliable conclusions. However, this is definitely not the case in our data. 
Hence, I would like to state that our linear models may not be the best models to show linear regression. We could have transformed the data to make the data normal, but that is not something I know how to do yet. 


###6. One-sample T-test
```{r}
t.test(vehicle$numkilled,m=1)
```
A one sample t test is generally use to compare the mean value of a particular variable to a pre-determined mean value. The functions takes in a variable name, and a set mean value [m = a constant].

The code creates a one-sample t-test, comparing the number of people killed in an accident with mean value=1. This t test will see if on average, an accident results in one person dying or not.

As we can see, the the p-value is <0.05, therefore the test is statistically significant. the null hypothesis value does not lie in the 95% confidence interval range, again showing that the test is statistically significant. We thus accept the alternate hypothesis that the true mean of the number of people being killed in an accident is not 1 [as one would hopefully expect], but is significantly lower than that [mean of x = 0.0022].




###7. Two-sample T-test
```{r}
t.test(vehicle$numkilled,vehicle$numinjured, na.rm=TRUE)
```

A two-sample t test is typically used to whether there is a difference between the true mean of two variables. 
The code creates a t test between the number of people killed and number of people injured in an accident.
Based off of the results of the T test, the means between the number of people killed an injured in an accident are significantly different. The P value is < .05 and the T value is relatively large, showing that we can reject the null hypothesis. The 95% confidence interval does not contain the null hypothesis value, which is 0, showing again that our results are statistically significant. 

###8. Correlation test

```{r}
cor.test(vehicle$numinjured,vehicle$hour)

```

The correlation test is used  used to assess the strength and direction of the linear relationships between pairs of variables.
The code creates a correlation test between the number of people injured vs the hour of the day. 
The test shows that there is a very small, yet positive correlation between the number of people killed vs hour of the day. That means after every hour during the day, your chance of being injured in an accident increased by 0.01609326


# I was unable to do more statistical tests, such as NCV test, since my data set is not suited for those tests. Hence, I will be using the polarization data set to demonstrate the 2 more tests required.

```{r,include=FALSE}
hdw<-read_csv("house-dwnom.csv",col_names = TRUE)

hdw<-mutate(hdw,year=1789+(cong-1)*2)

hdw1876<-hdw%>%filter(year>=1876)

hdw1876<-mutate(hdw1876,twoparty=ifelse(ptycode!=100 & ptycode!=200, NA_integer_,
                                        ifelse(ptycode==200,1,0)))

hdw1876<-mutate(hdw1876,repordem=ifelse(twoparty==0,"Democrat","Republican"))

hdw1876<-hdw1876%>%group_by(cong,repordem)%>%
  mutate(ptymean=mean(dw1))

hdw1876<-hdw1876%>%group_by(cong)%>%
  mutate(congmean=mean(dw1))

hdw1876 %>% filter(!is.na(repordem)) %>%
  ggplot(aes(year,ptymean))+
  geom_line(aes(col=repordem))+
  geom_line(aes(year,congmean))+
  scale_color_manual(values=c('blue',"red"))+
  theme_tufte()+
  scale_x_continuous(breaks=seq(1877,2013,10))+
  labs(x="Year",y="Men DW1 score",title="Mean-DW1 scores over time and by party",color="Party",
       caption="Note:black line is mean DW1 for all members in a Congress")

hdw1876_byYear<- hdw1876%>% group_by(year)%>%
  summarise(congmean=mean(dw1))

hdw1876_dems <- hdw1876%>%
  filter(repordem=="Democrat")%>%group_by(year)%>%
  summarise(demmean=mean(dw1))

hdw1876_reps <- hdw1876%>%
  filter(repordem=="Republican")%>%group_by(year)%>%
  summarise(repmean=mean(dw1))

hdw1876cong<- left_join(hdw1876_byYear,hdw1876_dems,by=c("year"))

hdw1876cong<- left_join(hdw1876cong,hdw1876_reps,by=c("year"))

hdw1876cong<-mutate(hdw1876cong,polarization=abs(demmean-repmean))


```
### 9. ncvTest

```{r,include=FALSE}
model_polar<-lm(polarization~year,hdw1876cong)
summary(model_polar)
```

```{r}
ncvTest(model_polar)
```

an ncvTest is used to see whether the distribution of data is homoskedastic or not. It takes in the regression model we created as a variable. 
The model I used is the polarization model, which shows how polarization has been affected by year [since 1876]. 

The code above creates an ncv test to see whether the data is distributed homoskedastically or not. the p-value for ncvTest is <0.05, therefore the data is also not homoskedastic. I believe the p-value is strong enough as well.

### 10. Confint
```{r}
confint(model_polar,"year",level=0.95)
```

A confint is usually used to figure out a range of values, derived from sample statistics, that is likely to contain the value of an unknown population parameter.

The code finds the 95% confidence interval of the "hour variable in our linear regression model. Looking at the confidence interval, we can say that we have 95% confidence that the true change in polarization due to year lies within the range of values given. In other words, we have 95% confidence, that for every increase in year, the change in polarization would be from the range given.

